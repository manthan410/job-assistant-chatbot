{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkfDeujMC74X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d6c9d53c-2ea0-4643-94db-b6956bbe17c6"
      },
      "source": [
        "import spacy\n",
        "spacy.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.5.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "z_LESGYvYH8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUDsY8hN8F9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ee521f-6860-4b2f-bd49-22afb22f05c7"
      },
      "source": [
        "!wget https://github.com/kbrajwani/resume_parser/releases/download/traindata/traindata.json -O traindata.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-26 06:37:26--  https://github.com/kbrajwani/resume_parser/releases/download/traindata/traindata.json\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/287977460/578669df-96db-41ea-9ff5-3da384b45b08?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230326%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230326T063726Z&X-Amz-Expires=300&X-Amz-Signature=66ec0c800eda0ed46e35af42f9c39504cddc563eb53dd245e3972c0e16abcae6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=287977460&response-content-disposition=attachment%3B%20filename%3Dtraindata.json&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-03-26 06:37:26--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/287977460/578669df-96db-41ea-9ff5-3da384b45b08?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230326%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230326T063726Z&X-Amz-Expires=300&X-Amz-Signature=66ec0c800eda0ed46e35af42f9c39504cddc563eb53dd245e3972c0e16abcae6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=287977460&response-content-disposition=attachment%3B%20filename%3Dtraindata.json&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4449383 (4.2M) [application/octet-stream]\n",
            "Saving to: ‘traindata.json’\n",
            "\n",
            "traindata.json      100%[===================>]   4.24M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-03-26 06:37:27 (58.8 MB/s) - ‘traindata.json’ saved [4449383/4449383]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOpOk88O7pfx"
      },
      "source": [
        "import json\n",
        "import logging\n",
        "import re\n",
        "import itertools\n",
        "\n",
        "def check(t1 , l1):\n",
        "  _range = list(itertools.chain(*[list(range(i[0],i[1])) for i in l1]))\n",
        "  if set(list(range(t1[0],t1[1]))).intersection(_range):\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "def trim_entity_spans(data: list) -> list:\n",
        "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
        "\n",
        "    Args:\n",
        "        data (list): The data to be cleaned in spaCy JSON format.\n",
        "\n",
        "    Returns:\n",
        "        list: The cleaned data.\n",
        "    \"\"\"\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in data:\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for start, end, label in entities:\n",
        "            valid_start = start\n",
        "            valid_end = end\n",
        "            while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                    text[valid_start]):\n",
        "                valid_start += 1\n",
        "            while valid_end > 1 and invalid_span_tokens.match(\n",
        "                    text[valid_end - 1]):\n",
        "                valid_end -= 1\n",
        "            valid_entities.append([valid_start, valid_end, label])\n",
        "        cleaned_data.append([text,{\"entities\": valid_entities}])\n",
        "\n",
        "    return cleaned_data\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In following code we are filtering the data for specific label Degree as we are only training for 1 entity. You can change as per your requirements."
      ],
      "metadata": {
        "id": "HJDFWrPxpD-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
        "    try:\n",
        "        training_data = []\n",
        "        lines = []\n",
        "        with open(dataturks_JSON_FilePath, 'r', encoding=\"utf8\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            data = json.loads(line)\n",
        "            text = data['content']\n",
        "            entities = []\n",
        "            l1 = []\n",
        "            if data['annotation'] is not None:\n",
        "                for annotation in data['annotation']:\n",
        "                    # only a single point in text annotation.\n",
        "\n",
        "                    if annotation['label'] == ['Degree']:\n",
        "                      point = annotation['points'][0]\n",
        "                      if check((point['start'], point['end']),l1):\n",
        "                        l1.append((point['start'], point['end']))\n",
        "                        labels = annotation['label']\n",
        "                        # handle both list of labels or a single label.\n",
        "                        if not isinstance(labels, list):\n",
        "                            labels = [labels]\n",
        "\n",
        "                        for label in labels:\n",
        "                            # dataturks indices are both inclusive [start, end]\n",
        "                            # but spacy is not [start, end)\n",
        "                            entities.append((\n",
        "                                point['start'],\n",
        "                                point['end'] + 1,\n",
        "                                label\n",
        "                            ))\n",
        "            if entities != []:\n",
        "              training_data.append((text, {\"entities\": entities}))\n",
        "        return training_data\n",
        "    except Exception:\n",
        "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath)\n",
        "        return None"
      ],
      "metadata": {
        "id": "Ab6NU29Zo4mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DATA = trim_entity_spans(convert_dataturks_to_spacy(\"traindata.json\"))"
      ],
      "metadata": {
        "id": "OuHfdfcNo6VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWxrGb2xVCXX"
      },
      "source": [
        "import re\n",
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "\n",
        "def convert_goldparse(dataturks_JSON_FilePath):\n",
        "    try:\n",
        "        training_data = []\n",
        "        lines = []\n",
        "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            data = json.loads(line)\n",
        "            text = data['content'].replace(\"\\n\", \" \")\n",
        "            entities = []\n",
        "            data_annotations = data['annotation']\n",
        "            if data_annotations is not None:\n",
        "                for annotation in data_annotations:\n",
        "                    point = annotation['points'][0]\n",
        "                    labels = annotation['label']\n",
        "                    if not isinstance(labels, list):\n",
        "                        labels = [labels]\n",
        "\n",
        "                    for label in labels:\n",
        "                        point_start = point['start']\n",
        "                        point_end = point['end']\n",
        "                        point_text = point['text']\n",
        "\n",
        "                        lstrip_diff = len(point_text) - \\\n",
        "                            len(point_text.lstrip())\n",
        "                        rstrip_diff = len(point_text) - \\\n",
        "                            len(point_text.rstrip())\n",
        "                        if lstrip_diff != 0:\n",
        "                            point_start = point_start + lstrip_diff\n",
        "                        if rstrip_diff != 0:\n",
        "                            point_end = point_end - rstrip_diff\n",
        "                        entities.append((point_start, point_end + 1, label))\n",
        "            training_data.append((text, {\"entities\": entities}))\n",
        "        return training_data\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Unable to process \" +\n",
        "                          dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
        "        return None\n",
        "\n",
        "def trim_entity_spans(data: list) -> list:\n",
        "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
        "    Args:\n",
        "        data (list): The data to be cleaned in spaCy JSON format.\n",
        "    Returns:\n",
        "        list: The cleaned data.\n",
        "    \"\"\"\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in data:\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for start, end, label in entities:\n",
        "            valid_start = start\n",
        "            valid_end = end if end <= len(text) else len(text)\n",
        "            try:\n",
        "              while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                      text[valid_start]):\n",
        "                  valid_start += 1\n",
        "              while valid_end > 1 and invalid_span_tokens.match(\n",
        "                      text[valid_end - 1]):\n",
        "                  valid_end -= 1\n",
        "            except Exception as e:\n",
        "              print(text,\"\\n\"*2,len(text), valid_start, valid_end)\n",
        "              raise e\n",
        "            valid_entities.append([valid_start, valid_end, label])\n",
        "        cleaned_data.append([text, {'entities': valid_entities}])\n",
        "    return cleaned_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7VhFA4vVCXY"
      },
      "source": [
        "import json\n",
        "## local file\n",
        "data = trim_entity_spans(TRAIN_DATA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzqv8FSlVCXa"
      },
      "source": [
        "TRAIN_DATA = data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQQcPiK7VCXa"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "# sentencizer = nlp.create_pipe(\"sentencizer\")\n",
        "# nlp.add_pipe(sentencizer)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "new_data = []\n",
        "for data in TRAIN_DATA:\n",
        "  text = data[0].replace(\"&lt;\",\"<\").replace(\"&gt;\",\">\").replace(\"&amp;\",\"&\")\n",
        "  annotation = data[1]\n",
        "  _label = pd.DataFrame(annotation['entities'],columns = ['start','end','label'])\n",
        "  doc = nlp(text.replace(\"\\n\",\" \"))\n",
        "  sentances = list(doc.sents)\n",
        "\n",
        "  paragraph = {'0':\"\"}\n",
        "  idx = 0\n",
        "  start = 0\n",
        "  find_start = 0\n",
        "  for sentance in sentances:\n",
        "    if len(paragraph[str(idx)].split(\" \"))+len(str(sentance).split(\" \")) < 280:\n",
        "      paragraph[str(idx)] += \" \"+ str(sentance)\n",
        "    else:\n",
        "      stop = text.replace(\"\\n\",\" \").find(str(sentance),find_start)\n",
        "      if _label[(_label['start']> start) & (_label['end'] < stop)].shape[0] != 0:\n",
        "        new_data.append([start,stop,text[start:stop],_label[(_label['start']> start) & (_label['end'] < stop)]])\n",
        "      start = stop\n",
        "      find_start = start+ len(str(sentance))\n",
        "      idx += 1\n",
        "      paragraph[str(idx)] = \" \"+ str(sentance)\n",
        "\n",
        "  if start == 0 :\n",
        "    new_data.append([start,len(text),text,_label])\n",
        "  else:\n",
        "    if _label[(_label['start']> start) & (_label['end'] < stop)].shape[0] != 0:\n",
        "      new_data.append([start,len(text),text[start:],_label[(_label['start']> start) & (_label['end'] < stop)]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJaX1GRPBaQS"
      },
      "source": [
        "correct_data = new_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPjGrbUuP4Vp"
      },
      "source": [
        "# from spacy.gold import biluo_tags_from_offsets\n",
        "from spacy.training import offsets_to_biluo_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcifpfxACSM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2d49df-13ea-4261-96d8-0b256cb2e396"
      },
      "source": [
        "sentences = []\n",
        "with open(\"resume_train.conll\",\"w\") as f:\n",
        "  for idx in range(len(correct_data)):\n",
        "      # idx = 2\n",
        "    f.write(\"-DOCSTART- -X- O O\\n\")\n",
        "    start = correct_data[idx][0]\n",
        "    text = correct_data[idx][2]\n",
        "    entities = correct_data[idx][3][['start','end','label']]\n",
        "    entities['start'] = entities['start'] - start\n",
        "    entities['end'] = entities['end'] - start\n",
        "    entities = tuple(entities.values.tolist())\n",
        "    doc = nlp(text)\n",
        "    try:\n",
        "      tags = offsets_to_biluo_tags(doc, entities)\n",
        "    except:\n",
        "      print(\"error\")\n",
        "      continue\n",
        "    ner_info = list(zip(doc, tags))\n",
        "    tokens = []\n",
        "    # print(correct_data[idx][3])\n",
        "    for n, i in enumerate(ner_info):\n",
        "      if i[0].text.strip() == \"\":\n",
        "        continue\n",
        "      if i[1].strip() == \"-\":\n",
        "        # print(idx)\n",
        "\n",
        "        try:\n",
        "          f.write(i[0].text.strip()+\"\\t\"+'O'+\"\\n\")\n",
        "        except UnicodeEncodeError:\n",
        "          pass\n",
        "          # print(\"text \\t \" , i[0].text)\n",
        "          # f.write(i[0].text.encode('ascii', 'ignore').decode('ascii').strip()+\"\\t\"+'O'+\"\\n\")\n",
        "      else:\n",
        "        try:\n",
        "          f.write(i[0].text.strip()+\"\\t\"+i[1]+\"\\n\")\n",
        "        except UnicodeEncodeError:\n",
        "          pass\n",
        "          # print(\"text \\t \",i[0].text,\"\\t\" , i[0].text.encode('ascii', 'ignore').decode('ascii'))\n",
        "          # f.write(i[0].text.encode('ascii', 'ignore').decode('ascii').strip()+\"\\t\"+i[1]+\"\\n\")\n",
        "\n",
        "      # print( i[0].string,i[1])\n",
        "\n",
        "    # print(\"\\t\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Nida Khan\n",
            "Tech Support Executive - Teleperformance...\" with entities \"([552, 609, 'Degree'],)\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
            "Roles and Responsibilities:\n",
            "• Created the test ca...\" with entities \"([718, 727, 'Degree'],)\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Karthik G V\n",
            "Program Manager, Product Manager, Prod...\" with entities \"([1103, 1132, 'Degree'],)\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"B. Gokul\n",
            "Gokul, Uttar Pradesh - Email me on Indeed...\" with entities \"([626, 631, 'Degree'],)\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Keshav Dhawale\n",
            "3 TCS Security guard Access Control...\" with entities \"([872, 874, 'Degree'],)\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The solution offers real-time, secure access to\n",
            "Fi...\" with entities \"([1764, 1769, 'Degree'], [1690, 1707, 'Degree'])\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Puran Mal\n",
            "Jaipur, Rajasthan - Email me on Indeed: ...\" with entities \"([174, 183, 'Degree'],)\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Hemal Patel\n",
            "Sales Manager - real estate firm Kb\n",
            "\n",
            "M...\" with entities \"([1344, 1349, 'Degree'], [1253, 1285, 'Degree'], [...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Neha Lakhanpal\n",
            "MBA- Marketing with 6 years of expe...\" with entities \"([15, 18, 'Degree'],)\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
            "EDUCATION\n",
            "\n",
            "Diploma in Business Management\n",
            "\n",
            "ICFAI ...\" with entities \"([175, 207, 'Degree'], [82, 121, 'Degree'], [12, 4...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Mayuresh Patil\n",
            "Mumbai, Maharashtra - Email me on I...\" with entities \"([1152, 1163, 'Degree'], [1068, 1106, 'Degree'], [...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Aboli Patil\n",
            "Jalgaon, Maharashtra - Email me on Ind...\" with entities \"([720, 723, 'Degree'], [626, 631, 'Degree'], [507,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Kalyani Parihar\n",
            "Akola, Maharashtra - Email me on I...\" with entities \"([422, 427, 'Degree'],)\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Shreya Biswas\n",
            "Kolkata, West Bengal - Email me on I...\" with entities \"([960, 990, 'Degree'], [309, 317, 'Degree'])\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Rohit Kumar\n",
            "Patna, Bihar - Email me on Indeed: ind...\" with entities \"([1596, 1604, 'Degree'], [1431, 1436, 'Degree'], [...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
            "Designated as Area Sales Executive\n",
            "\n",
            "Colgate Palmo...\" with entities \"([920, 922, 'Degree'], [865, 867, 'Degree'])\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rD4pNuyAZBXG",
        "outputId": "a9f32dd4-ec4e-40bf-ee3b-49064efa84f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Merchandising.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSDKsdX2GBde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6b3302-597d-4cf9-c868-f5d2f605d3cf"
      },
      "source": [
        "!python -m spacy convert resume_train.conll ./ -t json -n 1 -c iob\n",
        "# !python -m spacy convert NER_DEV.tsv ./ -t json -n 1 -c iob\n",
        "# !python -m spacy convert NER_TEST.tsv ./ -t json -n 1 -c iob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-03-26 06:41:32.003511: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-26 06:41:32.003691: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-26 06:41:32.003717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-26 06:41:34.508434: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
            "\u001b[38;5;3m⚠ Document delimiters found, automatic document segmentation with `-n`\n",
            "disabled.\u001b[0m\n",
            "\u001b[38;5;3m⚠ No sentence boundaries found. Use `-s` to automatically segment\n",
            "sentences.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (1 documents): resume_train.json\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGAyEyVPGBQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a81eb40-d1ca-44b2-91fd-4cf76a8d62a3"
      },
      "source": [
        "!python -m spacy convert resume_train.json ./ -t spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-03-26 06:41:41.388768: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-26 06:41:41.388895: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-26 06:41:41.388919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-26 06:41:43.168346: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;2m✔ Generated output file (338 documents): resume_train.spacy\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile base_config.cfg\n",
        "[paths]\n",
        "train = null\n",
        "dev = null\n",
        "vectors = null\n",
        "[system]\n",
        "gpu_allocator = null\n",
        "\n",
        "[nlp]\n",
        "lang = \"en\"\n",
        "pipeline = [\"tok2vec\",\"ner\"]\n",
        "batch_size = 1000\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.tok2vec]\n",
        "factory = \"tok2vec\"\n",
        "\n",
        "[components.tok2vec.model]\n",
        "@architectures = \"spacy.Tok2Vec.v2\"\n",
        "\n",
        "[components.tok2vec.model.embed]\n",
        "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "attrs = [\"NORM\", \"PREFIX\", \"SUFFIX\", \"SHAPE\"]\n",
        "rows = [5000, 1000, 2500, 2500]\n",
        "include_static_vectors = false\n",
        "\n",
        "[components.tok2vec.model.encode]\n",
        "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
        "width = 96\n",
        "depth = 4\n",
        "window_size = 1\n",
        "maxout_pieces = 3\n",
        "\n",
        "[components.ner]\n",
        "factory = \"ner\"\n",
        "\n",
        "[components.ner.model]\n",
        "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
        "state_type = \"ner\"\n",
        "extra_state_tokens = false\n",
        "hidden_width = 64\n",
        "maxout_pieces = 2\n",
        "use_upper = true\n",
        "nO = null\n",
        "\n",
        "[components.ner.model.tok2vec]\n",
        "@architectures = \"spacy.Tok2VecListener.v1\"\n",
        "width = ${components.tok2vec.model.encode.width}\n",
        "\n",
        "[corpora]\n",
        "\n",
        "[corpora.train]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.train}\n",
        "max_length = 0\n",
        "\n",
        "[corpora.dev]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.dev}\n",
        "max_length = 0\n",
        "\n",
        "[training]\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_words.v1\"\n",
        "discard_oversize = false\n",
        "tolerance = 0.2\n",
        "\n",
        "[training.batcher.size]\n",
        "@schedules = \"compounding.v1\"\n",
        "start = 100\n",
        "stop = 1000\n",
        "compound = 1.001\n",
        "\n",
        "[initialize]\n",
        "vectors = ${paths.vectors}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDPhvgURe_Dr",
        "outputId": "947fa237-bb7b-4220-c91e-e11754ab053d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing base_config.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk7B79liJyI3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "501916d1-6908-419d-e4b6-ee63deee93c1"
      },
      "source": [
        "!python -m spacy init fill-config base_config.cfg config_spacy.cfg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-03-26 06:41:50.899489: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-26 06:41:50.899605: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-26 06:41:50.899622: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-26 06:41:52.691597: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config_spacy.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config_spacy.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvgA6YOR--oj"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSakTvIjKj0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24fc770a-0b50-42b3-a7a8-ea3f6623a746"
      },
      "source": [
        "!python -m spacy debug data config_spacy.cfg --paths.train /content/resume_train.spacy --paths.dev /content/resume_train.spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-03-26 06:41:57.783711: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-26 06:41:57.783844: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-26 06:41:57.783868: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-26 06:41:59.985707: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[1m\n",
            "============================ Data file validation ============================\u001b[0m\n",
            "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
            "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
            "\u001b[1m\n",
            "=============================== Training stats ===============================\u001b[0m\n",
            "Language: en\n",
            "Training pipeline: tok2vec, ner\n",
            "338 training docs\n",
            "338 evaluation docs\n",
            "\u001b[38;5;3m⚠ 253 training examples also in evaluation data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples to train a new pipeline (338)\u001b[0m\n",
            "\u001b[1m\n",
            "============================== Vocab & Vectors ==============================\u001b[0m\n",
            "\u001b[38;5;4mℹ 74149 total word(s) in the data (8824 unique)\u001b[0m\n",
            "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
            "\u001b[1m\n",
            "========================== Named Entity Recognition ==========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 1 label(s)\u001b[0m\n",
            "0 missing value(s) (tokens with '-' label)\n",
            "\u001b[38;5;2m✔ Good amount of examples for all labels\u001b[0m\n",
            "\u001b[38;5;2m✔ Examples without occurrences available for all labels\u001b[0m\n",
            "\u001b[38;5;2m✔ No entities consisting of or starting/ending with whitespace\u001b[0m\n",
            "\u001b[38;5;2m✔ No entities crossing sentence boundaries\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Summary ==================================\u001b[0m\n",
            "\u001b[38;5;2m✔ 6 checks passed\u001b[0m\n",
            "\u001b[38;5;3m⚠ 2 warnings\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmv-MDuaaM0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db81337-32ce-4568-af04-2dc8e830ecae"
      },
      "source": [
        "!python -m spacy train config_spacy.cfg --paths.train /content/resume_train.spacy --paths.dev /content/resume_train.spacy --output ./out/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-03-26 07:01:38.389334: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-26 07:01:38.389483: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-26 07:01:38.389503: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-26 07:01:40.575023: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;4mℹ Saving to output directory: out\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-03-26 07:01:41,589] [INFO] Set up nlp object from config\n",
            "[2023-03-26 07:01:41,604] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2023-03-26 07:01:41,611] [INFO] Created vocabulary\n",
            "[2023-03-26 07:01:41,612] [INFO] Finished initializing nlp object\n",
            "[2023-03-26 07:01:44,581] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     54.00    0.00    0.00    0.00    0.00\n",
            "  0     200        932.13   2144.70   11.30   11.87   10.78    0.11\n",
            "  1     400         31.98    331.98   74.22   72.13   76.45    0.74\n",
            "  1     600         69.43    210.27   80.21   84.86   76.05    0.80\n",
            "  2     800         83.77    150.46   86.44   88.33   84.63    0.86\n",
            "  3    1000         99.03    145.64   91.22   92.24   90.22    0.91\n",
            "  3    1200         94.51    104.67   91.69   89.85   93.61    0.92\n",
            "  4    1400        128.72    104.52   92.97   93.54   92.42    0.93\n",
            "  4    1600        120.84    107.81   94.95   94.29   95.61    0.95\n",
            "  5    1800         99.04     66.24   95.23   94.85   95.61    0.95\n",
            "  6    2000         87.65     71.13   96.18   96.96   95.41    0.96\n",
            "  6    2200        198.55     71.66   95.41   95.41   95.41    0.95\n",
            "  7    2400        440.82     74.40   95.21   95.21   95.21    0.95\n",
            "  8    2600        117.56     62.05   97.43   96.48   98.40    0.97\n",
            "  8    2800         91.31     42.29   95.39   93.82   97.01    0.95\n",
            "  9    3000        126.27     54.28   96.59   96.98   96.21    0.97\n",
            " 10    3200        161.84     63.07   96.91   96.81   97.01    0.97\n",
            " 11    3400        531.96     80.16   97.72   97.05   98.40    0.98\n",
            " 12    3600        202.95     79.32   98.20   98.20   98.20    0.98\n",
            " 14    3800        215.17     65.62   98.51   98.21   98.80    0.99\n",
            " 16    4000        222.84     84.98   99.20   99.20   99.20    0.99\n",
            " 19    4200        306.91     56.10   98.59   99.59   97.60    0.99\n",
            " 21    4400        575.66     48.77   99.10   99.20   99.00    0.99\n",
            " 23    4600        153.29     49.27   99.20   99.40   99.00    0.99\n",
            " 26    4800        225.45     49.29   98.91   98.42   99.40    0.99\n",
            " 28    5000        489.73     49.00   99.10   98.81   99.40    0.99\n",
            " 31    5200        132.02     50.91   99.20   99.20   99.20    0.99\n",
            " 33    5400        164.65     49.65   99.00   99.40   98.60    0.99\n",
            " 35    5600        211.42     50.82   99.10   99.40   98.80    0.99\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "out/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1APfihbJXB0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJkObgmtHf3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3a3bd3-e105-4435-fb09-681a2b936a9c"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"/content/out/model-last/\")\n",
        "text = [\n",
        "'''Active member of IIIT Committee in Third year\\n\\nSangli, Maharashtra\\n\\nI wish to use my knowledge, skills and conceptual understanding to create excellent team\\nenvironments and work consistently achieving organization objectives believes in taking initiative\\nand work to excellence in my work.\\n\\nWORK EXPERIENCE\\n\\nActive member of IIIT Committee in Third year\\n\\nCisco Networking -  Kanpur, Uttar Pradesh\\n\\norganized by Techkriti IIT Kanpur and Azure Skynet.\\nPERSONALLITY TRAITS:\\n• Quick learning ability\\n• hard working\\n\\nEDUCATION\\n\\nPG-DAC\\n\\nCDAC ACTS\\n\\n2017\\n\\nBachelor of Engg in Information Technology\\n\\nShivaji University Kolhapur -  Kolhapur, Maharashtra\\n\\n2016\\n\\nSKILLS\\n\\nDatabase (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\"'''\n",
        "]\n",
        "for doc in nlp.pipe(text, disable=[\"tagger\", \"parser\"]):\n",
        "    print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Bachelor of Engg in Information Technology', 'Degree')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MBVJbdHFtlSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}